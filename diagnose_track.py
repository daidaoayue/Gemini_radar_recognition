"""
èˆªè¿¹åˆ†æ”¯å•ç‹¬è¯Šæ–­è„šæœ¬
ç›®çš„ï¼šæµ‹è¯•èˆªè¿¹ç‰¹å¾æœ¬èº«çš„åˆ†ç±»èƒ½åŠ›
å¦‚æœèˆªè¿¹åˆ†æ”¯å•ç‹¬åªèƒ½è¾¾åˆ° 60-70%ï¼Œé‚£èåˆæ—¶å®ƒä¼šæ‹–ç´¯ RD åˆ†æ”¯
"""

import os
import sys
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

torch.backends.cudnn.benchmark = True

from data_loader_fusion import FusionDataLoader

print("âœ… æ¨¡å—åŠ è½½å®Œæˆ")


class TrackOnlyNet(nn.Module):
    """çº¯èˆªè¿¹åˆ†ç±»ç½‘ç»œ - è¯Šæ–­èˆªè¿¹ç‰¹å¾çš„åŒºåˆ†èƒ½åŠ›"""
    def __init__(self, num_classes=6):
        super(TrackOnlyNet, self).__init__()
        
        # èˆªè¿¹åˆ†æ”¯ (ä¸èåˆæ¨¡å‹ç›¸åŒ)
        self.track_net = nn.Sequential(
            nn.Conv1d(6, 64, kernel_size=5, padding=2),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Conv1d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            
            nn.AdaptiveMaxPool1d(1),
            nn.Flatten()
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x_track):
        feat = self.track_net(x_track)
        return self.classifier(feat)


def main():
    DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    
    # --- é…ç½® ---
    RD_TRAIN_DIR = "./dataset/train/2026-1-14/train"
    RD_VAL_DIR = "./dataset/train/2026-1-14/val"
    TRACK_DIR = "./dataset/track_test/"
    # TRACK_DIR =  "../Preprocess/KDEæµ‹é‡æ•°æ®é›†/"

    

    BATCH_SIZE = 32
    EPOCHS = 50
    
    # --- æ•°æ®åŠ è½½ ---
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    train_ds = FusionDataLoader(RD_TRAIN_DIR, TRACK_DIR, val=False)
    val_ds = FusionDataLoader(RD_VAL_DIR, TRACK_DIR, val=True)
    
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, 
                              num_workers=0, drop_last=True, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,
                            num_workers=0, pin_memory=True)
    
    # --- æ¨¡å‹ ---
    print("\nğŸ”§ åˆå§‹åŒ–çº¯èˆªè¿¹æ¨¡å‹...")
    model = TrackOnlyNet(num_classes=6).to(DEVICE)
    
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)
    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    best_acc = 0.0
    
    # --- è®­ç»ƒå¾ªç¯ ---
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ èˆªè¿¹åˆ†æ”¯è¯Šæ–­ | è®¾å¤‡: {DEVICE}")
    print(f"{'='*60}")
    print(f"{'Epoch':^6}|{'TrainLoss':^10}|{'TrainAcc':^10}|{'ValAcc':^10}|{'Best':^10}")
    print("-" * 50)
    
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f"Ep{epoch:02d}", ncols=50, leave=False)
        
        for x_rd, x_track, labels in pbar:
            # åªç”¨èˆªè¿¹æ•°æ®ï¼Œå¿½ç•¥ RD
            x_track = x_track.float().to(DEVICE)
            labels = labels.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(x_track)
            loss = loss_fn(outputs, labels)
            
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
        
        scheduler.step()
        
        avg_loss = train_loss / len(train_loader)
        train_acc = 100. * correct / total
        
        # --- éªŒè¯ ---
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for x_rd, x_track, labels in val_loader:
                x_track = x_track.float().to(DEVICE)
                labels = labels.to(DEVICE)
                
                outputs = model(x_track)
                _, predicted = outputs.max(1)
                val_total += labels.size(0)
                val_correct += predicted.eq(labels).sum().item()
        
        val_acc = 100. * val_correct / val_total
        
        mark = ""
        if val_acc > best_acc:
            best_acc = val_acc
            mark = "â­"
        
        print(f"{epoch:^6}|{avg_loss:^10.4f}|{train_acc:^9.2f}%|{val_acc:^9.2f}%|{best_acc:^9.2f}% {mark}")
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è¯Šæ–­ç»“æœï¼šèˆªè¿¹åˆ†æ”¯æœ€ä½³éªŒè¯å‡†ç¡®ç‡ = {best_acc:.2f}%")
    print(f"{'='*60}")
    
    if best_acc < 50:
        print("âš ï¸ èˆªè¿¹ç‰¹å¾åŒºåˆ†åº¦æä½ï¼Œå»ºè®®ï¼š")
        print("   1. æ£€æŸ¥èˆªè¿¹æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½")
        print("   2. æ£€æŸ¥èˆªè¿¹ä¸RDæ ·æœ¬çš„å¯¹é½æ˜¯å¦æ­£ç¡®")
        print("   3. å¯èƒ½éœ€è¦æ›´æ¢èˆªè¿¹ç‰¹å¾æå–æ–¹å¼")
    elif best_acc < 70:
        print("âš ï¸ èˆªè¿¹ç‰¹å¾åŒºåˆ†åº¦ä¸€èˆ¬ï¼Œå»ºè®®ï¼š")
        print("   1. åœ¨èåˆæ—¶å¤§å¹…é™ä½èˆªè¿¹åˆ†æ”¯æƒé‡")
        print("   2. æˆ–å®Œå…¨å†»ç»“RDåˆ†æ”¯ï¼Œåªè®©èˆªè¿¹å­¦ä¹ è¾…åŠ©ä¿¡æ¯")
    elif best_acc < 85:
        print("âœ“ èˆªè¿¹ç‰¹å¾æœ‰ä¸€å®šåŒºåˆ†åº¦ï¼Œå¯ä»¥å°è¯•èåˆ")
        print("   ä½†éœ€è¦è°¨æ…è®¾è®¡èåˆç­–ç•¥ï¼Œé¿å…æ‹–ç´¯RDåˆ†æ”¯")
    else:
        print("âœ… èˆªè¿¹ç‰¹å¾åŒºåˆ†åº¦è‰¯å¥½ï¼Œèåˆåº”è¯¥èƒ½æå‡æ€§èƒ½")


if __name__ == '__main__':
    main()
